{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26a69cc",
   "metadata": {
    "papermill": {
     "duration": 0.002673,
     "end_time": "2025-10-11T09:12:35.742808",
     "exception": false,
     "start_time": "2025-10-11T09:12:35.740135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## pip package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e55a6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T09:12:35.748406Z",
     "iopub.status.busy": "2025-10-11T09:12:35.747783Z",
     "iopub.status.idle": "2025-10-11T09:13:05.562786Z",
     "shell.execute_reply": "2025-10-11T09:13:05.561607Z"
    },
    "papermill": {
     "duration": 29.819638,
     "end_time": "2025-10-11T09:13:05.564614",
     "exception": false,
     "start_time": "2025-10-11T09:12:35.744976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install --system --no-index --find-links=/kaggle/input/rsna2025-packages/pip-packages -r /kaggle/input/rsna2025-pip-packages/requirements.txt --no-build-isolation \n",
    "!uv pip install --system --no-index --find-links=/kaggle/input/rsna2025-packages/pip-packages --force-reinstall numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e637e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T09:13:05.572433Z",
     "iopub.status.busy": "2025-10-11T09:13:05.572147Z",
     "iopub.status.idle": "2025-10-11T09:13:09.378416Z",
     "shell.execute_reply": "2025-10-11T09:13:09.377556Z"
    },
    "papermill": {
     "duration": 3.811468,
     "end_time": "2025-10-11T09:13:09.380013",
     "exception": false,
     "start_time": "2025-10-11T09:13:05.568545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!cp -r /kaggle/input/rsna2025-nnunet /kaggle/working/\n",
    "!rm -r /kaggle/working/rsna2025-nnunet/nnunetv2.egg-info;\n",
    "!cd /kaggle/working/rsna2025-nnunet; uv pip install --system . --no-build-isolation --no-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fb4b5",
   "metadata": {
    "papermill": {
     "duration": 0.002006,
     "end_time": "2025-10-11T09:13:09.384415",
     "exception": false,
     "start_time": "2025-10-11T09:13:09.382409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## apt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e64313e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T09:13:09.389642Z",
     "iopub.status.busy": "2025-10-11T09:13:09.389362Z",
     "iopub.status.idle": "2025-10-11T09:13:21.560003Z",
     "shell.execute_reply": "2025-10-11T09:13:21.559100Z"
    },
    "papermill": {
     "duration": 12.17492,
     "end_time": "2025-10-11T09:13:21.561390",
     "exception": false,
     "start_time": "2025-10-11T09:13:09.386470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!dpkg -i /kaggle/input/rsna2025-packages/libyaml-cpp0.7_0.7.0+dfsg-8build1_amd64.deb\n",
    "!dpkg -i /kaggle/input/rsna2025-packages/dcm2niix_1.0.20211006-1build1_amd64.deb\n",
    "!dpkg -i /kaggle/input/rsna2025-packages/libgdcm-tools_3.0.10-1build2_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08dc0fbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T09:13:21.566915Z",
     "iopub.status.busy": "2025-10-11T09:13:21.566694Z",
     "iopub.status.idle": "2025-10-11T09:13:22.550411Z",
     "shell.execute_reply": "2025-10-11T09:13:22.549558Z"
    },
    "papermill": {
     "duration": 0.988146,
     "end_time": "2025-10-11T09:13:22.551900",
     "exception": false,
     "start_time": "2025-10-11T09:13:21.563754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/rsna2025-src /kaggle/working; mv /kaggle/working/rsna2025-src /kaggle/working/src\n",
    "!cp -r /kaggle/input/rsna2025-scripts /kaggle/working; mv /kaggle/working/rsna2025-scripts /kaggle/working/scripts\n",
    "!ln -s /kaggle/input/rsna2025-configs /kaggle/working/configs\n",
    "\n",
    "# for rootutils\n",
    "!touch /kaggle/working/.project-root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1532f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T09:13:22.557634Z",
     "iopub.status.busy": "2025-10-11T09:13:22.557365Z",
     "iopub.status.idle": "2025-10-11T09:13:22.577110Z",
     "shell.execute_reply": "2025-10-11T09:13:22.576369Z"
    },
    "papermill": {
     "duration": 0.023962,
     "end_time": "2025-10-11T09:13:22.578286",
     "exception": false,
     "start_time": "2025-10-11T09:13:22.554324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rootutils\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "rootutils.setup_root(Path().resolve(), indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "root = os.environ[\"PROJECT_ROOT\"]\n",
    "os.chdir(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4b007e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T09:13:22.582795Z",
     "iopub.status.busy": "2025-10-11T09:13:22.582584Z",
     "iopub.status.idle": "2025-10-11T09:35:26.837224Z",
     "shell.execute_reply": "2025-10-11T09:35:26.835811Z"
    },
    "papermill": {
     "duration": 1324.259607,
     "end_time": "2025-10-11T09:35:26.839763",
     "exception": false,
     "start_time": "2025-10-11T09:13:22.580156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\r\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py:1459: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\r\n",
      "  warnings.warn(\r\n",
      "[OK] ONNX exported: /kaggle/working/trt/RSNA2025Trainer_moreDAv7__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model.onnx\r\n",
      "[10/11/2025-09:13:55] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 673, GPU 4830 (MiB)\r\n",
      "[10/11/2025-09:13:57] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +963, GPU +192, now: CPU 1599, GPU 5022 (MiB)\r\n",
      "[10/11/2025-09:13:58] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:13:58] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:13:58] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\r\n",
      "[10/11/2025-09:14:04] [TRT] [I] Compiler backend is used during engine build.\r\n",
      "[10/11/2025-09:16:17] [TRT] [I] Detected 1 inputs and 1 output network tensors.\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] Total Host Persistent Memory: 407504 bytes\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] Total Device Persistent Memory: 9591808 bytes\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] Max Scratch Memory: 136349184 bytes\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 226 steps to complete.\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 20.0326ms to assign 9 blocks to 226 nodes requiring 715538432 bytes.\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] Total Activation Memory: 715538432 bytes\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] Total Weights Memory: 203950752 bytes\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] Compiler backend is used during engine execution.\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] Engine generation completed in 140.855 seconds.\r\n",
      "[10/11/2025-09:16:19] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 1024 MiB\r\n",
      "[OK] TensorRT engine built via Python API: /kaggle/working/trt/RSNA2025Trainer_moreDAv7__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model_fp16.engine\r\n",
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\r\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py:1459: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\r\n",
      "  warnings.warn(\r\n",
      "[OK] ONNX exported: /kaggle/working/trt/nnUNetTrainerSkeletonRecall_more_DAv3__nnUNetResEncUNetMPlans__3d_fullres/fold_0/model.onnx\r\n",
      "[10/11/2025-09:16:42] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 663, GPU 5494 (MiB)\r\n",
      "[10/11/2025-09:16:44] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +964, GPU +192, now: CPU 1588, GPU 5686 (MiB)\r\n",
      "[10/11/2025-09:16:44] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:16:44] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:16:44] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\r\n",
      "[10/11/2025-09:16:51] [TRT] [I] Compiler backend is used during engine build.\r\n",
      "[10/11/2025-09:19:25] [TRT] [I] Detected 1 inputs and 1 output network tensors.\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] Total Host Persistent Memory: 406480 bytes\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] Total Device Persistent Memory: 10790400 bytes\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] Max Scratch Memory: 153392640 bytes\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 200 steps to complete.\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.6839ms to assign 9 blocks to 200 nodes requiring 804980736 bytes.\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] Total Activation Memory: 804980736 bytes\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] Total Weights Memory: 203965120 bytes\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] Compiler backend is used during engine execution.\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] Engine generation completed in 162.454 seconds.\r\n",
      "[10/11/2025-09:19:27] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 1152 MiB\r\n",
      "[OK] TensorRT engine built via Python API: /kaggle/working/trt/nnUNetTrainerSkeletonRecall_more_DAv3__nnUNetResEncUNetMPlans__3d_fullres/fold_0/model_fp16.engine\r\n",
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\r\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py:1459: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\r\n",
      "  warnings.warn(\r\n",
      "[OK] ONNX exported: /kaggle/working/trt/nnUNetTrainerSkeletonRecall_more_DAv3__nnUNetResEncUNetMPlans__3d_fullres/fold_1/model.onnx\r\n",
      "[10/11/2025-09:19:52] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 664, GPU 5494 (MiB)\r\n",
      "[10/11/2025-09:19:54] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +964, GPU +192, now: CPU 1588, GPU 5686 (MiB)\r\n",
      "[10/11/2025-09:19:54] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:19:54] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:19:54] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\r\n",
      "[10/11/2025-09:20:01] [TRT] [I] Compiler backend is used during engine build.\r\n",
      "[10/11/2025-09:22:36] [TRT] [I] Detected 1 inputs and 1 output network tensors.\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] Total Host Persistent Memory: 405776 bytes\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] Total Device Persistent Memory: 10790400 bytes\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] Max Scratch Memory: 153392640 bytes\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 200 steps to complete.\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.3948ms to assign 9 blocks to 200 nodes requiring 804980736 bytes.\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] Total Activation Memory: 804980736 bytes\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] Total Weights Memory: 203965120 bytes\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] Compiler backend is used during engine execution.\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] Engine generation completed in 163.699 seconds.\r\n",
      "[10/11/2025-09:22:38] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 1152 MiB\r\n",
      "[OK] TensorRT engine built via Python API: /kaggle/working/trt/nnUNetTrainerSkeletonRecall_more_DAv3__nnUNetResEncUNetMPlans__3d_fullres/fold_1/model_fp16.engine\r\n",
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\r\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py:1459: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\r\n",
      "  warnings.warn(\r\n",
      "[OK] ONNX exported: /kaggle/working/trt/nnUNetTrainerSkeletonRecall_more_DAv3__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model.onnx\r\n",
      "[10/11/2025-09:23:05] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 556, GPU 5494 (MiB)\r\n",
      "[10/11/2025-09:23:06] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +964, GPU +192, now: CPU 1588, GPU 5686 (MiB)\r\n",
      "[10/11/2025-09:23:07] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:23:07] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:23:07] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\r\n",
      "[10/11/2025-09:23:14] [TRT] [I] Compiler backend is used during engine build.\r\n",
      "[10/11/2025-09:25:49] [TRT] [I] Detected 1 inputs and 1 output network tensors.\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] Total Host Persistent Memory: 405776 bytes\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] Total Device Persistent Memory: 10790400 bytes\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] Max Scratch Memory: 153392640 bytes\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 200 steps to complete.\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.4054ms to assign 9 blocks to 200 nodes requiring 804980736 bytes.\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] Total Activation Memory: 804980736 bytes\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] Total Weights Memory: 203965120 bytes\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] Compiler backend is used during engine execution.\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] Engine generation completed in 164.509 seconds.\r\n",
      "[10/11/2025-09:25:52] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 1152 MiB\r\n",
      "[OK] TensorRT engine built via Python API: /kaggle/working/trt/nnUNetTrainerSkeletonRecall_more_DAv3__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model_fp16.engine\r\n",
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\r\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py:1459: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\r\n",
      "  warnings.warn(\r\n",
      "[OK] ONNX exported: /kaggle/working/trt/nnUNetTrainerSkeletonRecall_more_DAv3_ep800__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model.onnx\r\n",
      "[10/11/2025-09:26:16] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 662, GPU 5494 (MiB)\r\n",
      "[10/11/2025-09:26:18] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +964, GPU +192, now: CPU 1588, GPU 5686 (MiB)\r\n",
      "[10/11/2025-09:26:18] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:26:18] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:26:18] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\r\n",
      "[10/11/2025-09:26:25] [TRT] [I] Compiler backend is used during engine build.\r\n",
      "[10/11/2025-09:28:59] [TRT] [I] Detected 1 inputs and 1 output network tensors.\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] Total Host Persistent Memory: 405776 bytes\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] Total Device Persistent Memory: 10790400 bytes\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] Max Scratch Memory: 153392640 bytes\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 201 steps to complete.\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.5569ms to assign 9 blocks to 201 nodes requiring 804980736 bytes.\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] Total Activation Memory: 804980736 bytes\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] Total Weights Memory: 203965120 bytes\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] Compiler backend is used during engine execution.\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] Engine generation completed in 163.319 seconds.\r\n",
      "[10/11/2025-09:29:02] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 1152 MiB\r\n",
      "[OK] TensorRT engine built via Python API: /kaggle/working/trt/nnUNetTrainerSkeletonRecall_more_DAv3_ep800__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model_fp16.engine\r\n",
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\r\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py:1459: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\r\n",
      "  warnings.warn(\r\n",
      "[OK] ONNX exported: /kaggle/working/trt/RSNA2025Trainer_moreDAv6_SkeletonRecallW3TverskyBeta07__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model.onnx\r\n",
      "[10/11/2025-09:29:27] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 562, GPU 5494 (MiB)\r\n",
      "[10/11/2025-09:29:29] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +964, GPU +192, now: CPU 1588, GPU 5686 (MiB)\r\n",
      "[10/11/2025-09:29:30] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:29:30] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:29:30] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\r\n",
      "[10/11/2025-09:29:37] [TRT] [I] Compiler backend is used during engine build.\r\n",
      "[10/11/2025-09:32:10] [TRT] [I] Detected 1 inputs and 1 output network tensors.\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] Total Host Persistent Memory: 405072 bytes\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] Total Device Persistent Memory: 10790400 bytes\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] Max Scratch Memory: 153392640 bytes\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 201 steps to complete.\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.6418ms to assign 9 blocks to 201 nodes requiring 804980736 bytes.\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] Total Activation Memory: 804980736 bytes\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] Total Weights Memory: 203965120 bytes\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] Compiler backend is used during engine execution.\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] Engine generation completed in 163.004 seconds.\r\n",
      "[10/11/2025-09:32:13] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 1152 MiB\r\n",
      "[OK] TensorRT engine built via Python API: /kaggle/working/trt/RSNA2025Trainer_moreDAv6_SkeletonRecallW3TverskyBeta07__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model_fp16.engine\r\n",
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\r\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\r\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_helper.py:1459: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\r\n",
      "  warnings.warn(\r\n",
      "[OK] ONNX exported: /kaggle/working/trt/RSNA2025Trainer_moreDAv6_1_SkeletonRecallTverskyBeta07__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model.onnx\r\n",
      "[10/11/2025-09:32:37] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 656, GPU 5494 (MiB)\r\n",
      "[10/11/2025-09:32:39] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +964, GPU +192, now: CPU 1588, GPU 5686 (MiB)\r\n",
      "[10/11/2025-09:32:40] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:32:40] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\r\n",
      "[10/11/2025-09:32:40] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\r\n",
      "[10/11/2025-09:32:47] [TRT] [I] Compiler backend is used during engine build.\r\n",
      "[10/11/2025-09:35:21] [TRT] [I] Detected 1 inputs and 1 output network tensors.\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] Total Host Persistent Memory: 405776 bytes\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] Total Device Persistent Memory: 10790400 bytes\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] Max Scratch Memory: 153392640 bytes\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 200 steps to complete.\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 18.3415ms to assign 9 blocks to 200 nodes requiring 804980736 bytes.\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] Total Activation Memory: 804980736 bytes\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] Total Weights Memory: 203965120 bytes\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] Compiler backend is used during engine execution.\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] Engine generation completed in 163.715 seconds.\r\n",
      "[10/11/2025-09:35:24] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 1152 MiB\r\n",
      "[OK] TensorRT engine built via Python API: /kaggle/working/trt/RSNA2025Trainer_moreDAv6_1_SkeletonRecallTverskyBeta07__nnUNetResEncUNetMPlans__3d_fullres/fold_all/model_fp16.engine\r\n"
     ]
    }
   ],
   "source": [
    "!python scripts/convert_nnunet_to_onnx_trt.py --model-dir /kaggle/input/nnunet-vessel-grouping-da7  --fold all --checkpoint-name checkpoint_final.pth --out-dir /kaggle/working/trt --fp16\n",
    "!python scripts/convert_nnunet_to_onnx_trt.py --model-dir /kaggle/input/nnunet-da3-sklr-with-all  --fold 0 --checkpoint-name checkpoint_final.pth --out-dir /kaggle/working/trt --fp16\n",
    "!python scripts/convert_nnunet_to_onnx_trt.py --model-dir /kaggle/input/nnunet-da3-sklr-with-all  --fold 1 --checkpoint-name checkpoint_final.pth --out-dir /kaggle/working/trt --fp16\n",
    "!python scripts/convert_nnunet_to_onnx_trt.py --model-dir /kaggle/input/nnunet-da3-sklr-with-all  --fold all --checkpoint-name checkpoint_final.pth --out-dir /kaggle/working/trt --fp16\n",
    "!python scripts/convert_nnunet_to_onnx_trt.py --model-dir /kaggle/input/nnunet-da3-sklr-ep800  --fold all --checkpoint-name checkpoint_final.pth --out-dir /kaggle/working/trt --fp16\n",
    "!python scripts/convert_nnunet_to_onnx_trt.py --model-dir /kaggle/input/nnunet-da6-sklr-w3-tv07  --fold all --checkpoint-name checkpoint_final.pth --out-dir /kaggle/working/trt --fp16\n",
    "!python scripts/convert_nnunet_to_onnx_trt.py --model-dir /kaggle/input/nnunet-da6-1-sklr-tv07  --fold all --checkpoint-name checkpoint_final.pth --out-dir /kaggle/working/trt --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9137a8",
   "metadata": {
    "papermill": {
     "duration": 0.006295,
     "end_time": "2025-10-11T09:35:26.859359",
     "exception": false,
     "start_time": "2025-10-11T09:35:26.853064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13851420,
     "isSourceIdPinned": false,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 8091323,
     "sourceId": 12974092,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8407675,
     "sourceId": 13267539,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8407765,
     "sourceId": 13267672,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8407830,
     "sourceId": 13267764,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8408329,
     "sourceId": 13268490,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8211597,
     "sourceId": 13268619,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8211582,
     "sourceId": 13307077,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8211598,
     "sourceId": 13307086,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8449523,
     "sourceId": 13327553,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8211584,
     "sourceId": 13327619,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 260238394,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1375.794287,
   "end_time": "2025-10-11T09:35:27.185889",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-11T09:12:31.391602",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
